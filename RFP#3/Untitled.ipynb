{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76015f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  \n",
    "import random  \n",
    "\n",
    "# Environment parameters  \n",
    "grid_size = 3  \n",
    "goal_state = (0, 2)  # Flag cell  \n",
    "fire_state = (1, 2)  \n",
    "\n",
    "# Q-learning parameters  \n",
    "Q = np.zeros((grid_size, grid_size, 4))  # Q-table: 4 actions (0: up, 1: down, 2: left, 3: right)  \n",
    "alpha = 0.1  \n",
    "gamma = 0.9  \n",
    "epsilon = 1.0  \n",
    "epsilon_decay = 0.99  \n",
    "num_episodes = 1000  \n",
    "\n",
    "# Reward function  \n",
    "def get_reward(state):  \n",
    "    if state == goal_state:  \n",
    "        return 10  \n",
    "    elif state == fire_state:  \n",
    "        return -10  \n",
    "    else:  \n",
    "        return -1  \n",
    "\n",
    "# Choose action  \n",
    "def choose_action(state):  \n",
    "    if random.uniform(0, 1) < epsilon:  # Exploration  \n",
    "        return random.choice([0, 1, 2, 3])  # Random action  \n",
    "    else:  # Exploitation  \n",
    "        return np.argmax(Q[state[0], state[1]])  \n",
    "\n",
    "# Move robot based on action  \n",
    "def apply_action(state, action):  \n",
    "    if action == 0 and state[0] > 0:  # Up  \n",
    "        return (state[0] - 1, state[1])  \n",
    "    elif action == 1 and state[0] < grid_size - 1:  # Down  \n",
    "        return (state[0] + 1, state[1])  \n",
    "    elif action == 2 and state[1] > 0:  # Left  \n",
    "        return (state[0], state[1] - 1)  \n",
    "    elif action == 3 and state[1] < grid_size - 1:  # Right  \n",
    "        return (state[0], state[1] + 1)  \n",
    "    return state  # Invalid move  \n",
    "\n",
    "# Training Loop  \n",
    "for episode in range(num_episodes):  \n",
    "    state = (1, 0)  # Starting in the middle left cell  \n",
    "    while state != goal_state:  \n",
    "        action = choose_action(state)  \n",
    "        new_state = apply_action(state, action)  \n",
    "        reward = get_reward(new_state)  \n",
    "        \n",
    "        # Update Q-table  \n",
    "        Q[state[0], state[1], action] += alpha * (reward + gamma * np.max(Q[new_state[0], new_state[1]]) - Q[state[0], state[1], action])  \n",
    "        \n",
    "        state = new_state  \n",
    "    \n",
    "    # Decay epsilon  \n",
    "    epsilon *= epsilon_decay  \n",
    "    \n",
    "print(\"Training Completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1fa3632",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
